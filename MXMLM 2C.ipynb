{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabf5996",
   "metadata": {},
   "source": [
    "# Data processor for MXMLM v2\n",
    "Please read the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML, TEI, CMI/F and data handling\n",
    "from bs4 import BeautifulSoup # Hent BeautifulSoup-modulen (https://www.crummy.com/software/BeautifulSoup/) for XML\n",
    "from bs4 import Comment # BS4-addon for å håndtere kommentarer <!-- X -->\n",
    "import re # Regex\n",
    "import pandas as pd\n",
    "import collections # Facilitate dynamic dict\n",
    "\n",
    "# Time and date\n",
    "import datetime # Dates\n",
    "#from datetime import date\n",
    "import time # Time\n",
    "\n",
    "# File and folder handling\n",
    "import glob # The yeast of thought and mind\n",
    "import os # Filsystem; mapper, lagring, åpning, etc...\n",
    "#import shutil # Se os+\n",
    "import json # JSON!\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#Check if *keys (nested) exists in dict\n",
    "def keys_exists(element, *keys):\n",
    "    if not isinstance(element, dict):\n",
    "        raise AttributeError('keys_exists() expects dict as first argument.')\n",
    "    if len(keys) == 0:\n",
    "        raise AttributeError('keys_exists() expects at least two arguments, one given.')\n",
    "\n",
    "    _element = element\n",
    "    for key in keys:\n",
    "        try:\n",
    "            _element = _element[key]\n",
    "        except KeyError:\n",
    "            return False\n",
    "    return True\n",
    "# *MunchXMLmuncher* **X2**\n",
    "# MXMLMx2 reads and preprocesses files. First off, create subdirectories and locate our files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runFullProcessor = 1 # Set 0 to only run chrono and placename processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ac747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrono\n",
    "# Next, let's get the chrono if it exists, and then read the data out of it to a dict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a99855",
   "metadata": {},
   "source": [
    "## Chronology v2\n",
    "IMPORTANT. v2 **requires** modifications to the chronology file. All MM N/K/T objects must be replaced by No-MM_N/K/T. All objects must have excess spaces removed. PN objects are permitted to be formatted without leading zeroes following the prefix (PN99 will be read as PN0099). Do not under any circumstance attempt to use this script with the vanilla \"MM N 188\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47faff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasChrono,hasXMLs = False,False\n",
    "listofbaddies = []\n",
    "lookupChrono = sorted(glob.glob(\"*Kronologi_Munchs_brev*.xlsx\"), key=os.path.getmtime)\n",
    "x = len(lookupChrono)-1\n",
    "if x > -1:\n",
    "    print(\"Newest chronology file:\",lookupChrono[x])\n",
    "    chronologyFile = lookupChrono[x]\n",
    "    hasChrono = True\n",
    "    #shutil.copy2(chronologyFile, inputfolder+\"/kronologi.xlsx\")\n",
    "    #chronologyFile = inputfolder+\"/kronologi.xlsx\"\n",
    "    \n",
    "    chronology = pd.read_excel(chronologyFile).dropna(axis=1, how='all').dropna(axis=0, how='all').reset_index(drop=True)\n",
    "    chronology = chronology.fillna(\"N/A\")\n",
    "    \n",
    "    try:\n",
    "        if CHRONODICT:\n",
    "            print(\"CHRONODICT found with content\")\n",
    "        else:\n",
    "            print(\"CHRONODICT found without content\")\n",
    "    except:\n",
    "        CHRONODICT = collections.defaultdict(dict)\n",
    "        print(\"CHRONODICT created\")\n",
    "    for idx,row in chronology.iterrows():\n",
    "        mismatch = False\n",
    "        formattingError = False\n",
    "        document = chronology.iloc[idx]['Objektnr.']\n",
    "        rawdate = chronology.iloc[idx]['Dato']\n",
    "        #print(\"Document:\",document)\n",
    "        if rawdate != \"N/A\" and document != \"N/A\": # If date and documents are not N/A\n",
    "            #print(\"\\thas date and ID\")\n",
    "            document = document.replace(\" \",\"\")\n",
    "            # This section checks PN objects for compliance. Compliant PN objects have the PN prefix followed by 4 digits, total 6 chars.\n",
    "            # In cases where the object ID is too short, zeroes (0) are added immediately after the PN prefix until it complies.\n",
    "            # In cases where the object ID is too long, characters are removed from the end of the ID until it complies.\n",
    "            # This script will function until PN object IDs exceed 9999, meaning that there are 7 characters in the PN series instead of 6.\n",
    "            chkStr = document[0:2]\n",
    "            if chkStr == \"PN\":\n",
    "                # If the characters after the prefix are not ALL numeric, the object is skipped entirely.\n",
    "                if len(document) < 6:\n",
    "                    while len(document) < 6:\n",
    "                        document = document[0:2]+'0'+document[2:len(document)]\n",
    "                    #print(\"Extended\",document)\n",
    "                elif len(document) > 6:\n",
    "                    document = document[0:6]\n",
    "                    #print(\"Reduced\",document)\n",
    "            # This section checks No-MM_N/K objects for compliance. Compliant objects have the No prefix and a total of 11 characters.\n",
    "            # Procedure is identical to PN objects.\n",
    "            elif chkStr == \"No\":\n",
    "                \n",
    "                #if document[7:len(document)].isnumeric():\n",
    "                if len(document) < 11:\n",
    "                    while len(document) < 11:\n",
    "                        document = document[0:7]+\"0\"+document[7:len(document)]\n",
    "                   # print(\"Extended\",document)\n",
    "                elif len(document) > 11:\n",
    "                    if \",\" in document: # If you put a comma in the documentID, just remove EVERYTHING to the right of the first.\n",
    "                        splitD = document.split(\",\")\n",
    "                        document = splitD[0]\n",
    "                    #print(document[7])\n",
    "                    # Required to fix No-MM_N03101 and similar.\n",
    "                    if document[7] == \"0\":\n",
    "                        while len(document) > 11 and document[7] == \"0\":\n",
    "                            document = document[0:7]+document[8:len(document)] # Discards characters after prefix\n",
    "                    document = document[0:11] # Discards last n characters until 11 remain\n",
    "                #print(\"\\t\",document)\n",
    "        # If the last 4 characters are not ALL numeric, the object is skipped entirely. \n",
    "            if document[len(document)-4:].isnumeric() == False:\n",
    "                formattingError = True # Formatting error due to invalid filename.\n",
    "                listofbaddies.append(document)\n",
    "                print(document,\"is not a valid document ID and was excluded.\")\n",
    "        # If the document ID somehow is not 11 or 6 characters long, it is skipped entirely.\n",
    "            elif len(document) != 11 and len(document) != 6: # If string doesn't match with No-MM_N0000 or PN0000\n",
    "                #filenamePlain = \"Formatting error\"+filenamePlain # it is invalid.\n",
    "                formattingError = True # Formatting error due to invalid filename.\n",
    "                listofbaddies.append(document)\n",
    "                print(document,\"is not a valid document ID and was excluded.\")\n",
    "        # If the document ID is 11 or 6 characters long and the last 4 characters are numeric:\n",
    "            else:\n",
    "                #document = filenamePlain\n",
    "                if isinstance(rawdate,datetime.date): # If it's just a datetime object\n",
    "                    #dateobject = rawdate.strftime(\"%Y-%m-%d\")\n",
    "                    newdate = rawdate.strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    #print(document,rawdate)\n",
    "                    dateobject = str(rawdate) # Make sure it's string\n",
    "\n",
    "                    string4print = document+\" \"+dateobject\n",
    "\n",
    "                    #dateobject = dateobject.replace(\"–\",\"-\") # Replace long dash – with a normal dash - doesn't work\n",
    "                    if \".-\" in dateobject: # Like: 04.-05.1922.\n",
    "                        dateobject = dateobject.replace(\".-\",\"-\")\n",
    "\n",
    "                    if \"?\" in dateobject:\n",
    "                        dateobject = dateobject.replace(\"?\",\"\").strip(punctuation) # Remove n ?s and then also remove excess .\n",
    "\n",
    "                        dateobject = dateobject.replace(\"..\",\".\")\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "                    if \"-\" in dateobject:\n",
    "                        splitToFrom = dateobject.split(\"-\")\n",
    "                        fromDate = splitToFrom[0]\n",
    "                        toDate = splitToFrom[1]\n",
    "                        \n",
    "                        newFromDate = fromDate.split(\".\")\n",
    "                        newToDate = toDate.split(\".\")\n",
    "                        while (\"\" in newFromDate):\n",
    "                            newFromDate.remove(\"\")\n",
    "                        while (\"\" in newToDate):\n",
    "                            newToDate.remove(\"\")\n",
    "                        #Debug\n",
    "                        \n",
    "                        if len(newToDate) != len(newFromDate):\n",
    "                            mismatch = True\n",
    "                            print(\"Detected unbalanced date in\",document,len(newFromDate),\"vs\",len(newToDate),newFromDate,newToDate)\n",
    "                            if len(newToDate) > len(newFromDate):\n",
    "                                itemsToGet = len(newToDate)-len(newFromDate)-1\n",
    "                                #print(\"Items:\",itemsToGet)\n",
    "                                if itemsToGet == 1: # Get items 2 and 3\n",
    "                                    while itemsToGet < len(newToDate):\n",
    "                                        newFromDate.append(newToDate[itemsToGet])\n",
    "                                        itemsToGet+=1\n",
    "                                elif itemsToGet == 0: # Get last item\n",
    "                                    newFromDate.append(newToDate[len(newToDate)-1])\n",
    "                            else:\n",
    "                                print(\"WARNING: Unable to resolve instances where From date is more specific than To date!\")\n",
    "\n",
    "                        if isinstance(newFromDate,list):\n",
    "                            if len(newFromDate) > 1:\n",
    "                                newdateF = newFromDate[len(newFromDate)-1]\n",
    "                                for x in reversed(newFromDate):\n",
    "                                    if len(x) == 4:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        if len(x) == 2:\n",
    "                                            newdateF+=\".\"+x\n",
    "                                        else:\n",
    "                                            pass \n",
    "                            else:\n",
    "                                newdateF = newFromDate[0]\n",
    "                        else:\n",
    "                            newdateF = fromDate\n",
    "                        \n",
    "                        if isinstance(newToDate,list):\n",
    "                            if len(newToDate) > 1:\n",
    "                                newdateT = newToDate[len(newToDate)-1]\n",
    "                                for x in reversed(newToDate):\n",
    "                                    if len(x) == 4:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        if len(x) == 2:\n",
    "                                            newdateT+=\".\"+x\n",
    "                                        else:\n",
    "                                            pass \n",
    "                            else:\n",
    "                                newdateT = newFromDate[0]\n",
    "                        else:\n",
    "                            newdateT = toDate\n",
    "                        \n",
    "                        #print(document)\n",
    "                        newdate=newdateF+\"%\"+newdateT\n",
    "                        datetype = \"fromTo\"\n",
    "\n",
    "                    else:\n",
    "                        datelements = dateobject.split(\".\")\n",
    "                        newdate = datelements[len(datelements)-1]\n",
    "                        datetype = \"instance\"\n",
    "                        for x in reversed(datelements):\n",
    "                            if len(x) == 4:\n",
    "                                pass\n",
    "                            else:\n",
    "                                if len(x) == 2:\n",
    "                                    newdate+=\"-\"+x\n",
    "                                else:\n",
    "                                    break      \n",
    "                    #print(document,rawdate,\"(\"+newdate+\")\")\n",
    "                    if mismatch == True:\n",
    "                            print(\"Resolution:\",newdate,newFromDate,newToDate)\n",
    "                    CHRONODICT[document][\"date\"] = newdate # Set the dict item's date to newdate\n",
    "                    CHRONODICT[document][\"datetype\"] = datetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe140c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofbaddies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551f324",
   "metadata": {},
   "source": [
    "# Placenames v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Places\n",
    "# With the dates safely stored away, let's go for some place names.\n",
    "if os.path.isfile(\"ID_sted-verdier.xlsx\"):\n",
    "    print(\"Detected ID_sted-verdier.xlsx\")\n",
    "    if os.path.exists(\"xml-filer\"):\n",
    "        print(\"XML>CMIF placename augmentation enabled\")\n",
    "        listXMLfiles = glob.glob(\"xml-filer/**/*.xml\",recursive=True)\n",
    "        hasXMLs = True\n",
    "    else:\n",
    "        print(\"No XML files provided. XML>CMIF placename augmentation disabled\")\n",
    "else:\n",
    "    print(\"No ID_sted-verdier file provided. Skipping placename augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b36925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 0\n",
    "if hasXMLs == True: # If the XML files should be used for updating\n",
    "    # Check to see if CHRONODICT is alive or not. If it is, use it as destination. If it isn't, create it.\n",
    "    try:\n",
    "        if CHRONODICT:\n",
    "            print(\"CHRONODICT found with content\")\n",
    "        else:\n",
    "            print(\"CHRONODICT found without content\")\n",
    "    except:\n",
    "        CHRONODICT = collections.defaultdict(dict)\n",
    "        print(\"CHRONODICT created\")\n",
    "\n",
    "    addrsFoundInXMLs = [] # Make a simple list to hold the short names of every file we've found addresses for\n",
    "    xmlswithnoaddress = [] # Simple list for XMLs that have no address that can be printed later.\n",
    "    plainnames = [] # Simple list for plain names of XMLs that have been checked.\n",
    "    i=0\n",
    "    for item in listXMLfiles:\n",
    "        addrKey = \"NONE\" # Just in case\n",
    "        find_address = [] # Ensure that this list clears on start of each item\n",
    "        findFileName = item.split(\"\\\\\") # Make filepath a list\n",
    "        findFileName = findFileName[len(findFileName)-1] # Get the path destination file\n",
    "        \n",
    "        chkStr = findFileName[0:2] # Check the incoming ID - it's either PN+4 positions, or No+9 positions long.\n",
    "        if chkStr == \"PN\": # 6 positions\n",
    "            filenamePlain = findFileName[0:6] # Must be bounded to remove .xml as well as pagination from filename\n",
    "            #print(findFileName,filenamePlain)\n",
    "        elif chkStr == \"No\": # 11 positions\n",
    "            filenamePlain = findFileName[0:11] # Must be bounded to remove .xml as well as pagination from filename\n",
    "            #print(findFileName,filenamePlain)\n",
    "        else: # This doesn't occur unless you've got files that don't belong here\n",
    "            print(\"PROBLEM IN NAME PROCESSING\",findFileName,chkStr) \n",
    "        if filenamePlain not in plainnames: # For every unique name, add to plainnames\n",
    "            plainnames.append(str(filenamePlain)) # Just in case we need them later\n",
    "        print (\"\\r\",\"Progress:\",round(i/len(listXMLfiles)*100),\"%\", end='\\t')\n",
    "        #print (\"\\r\",n*0.1, end='')\n",
    "        #n+=1\n",
    "        if filenamePlain in addrsFoundInXMLs: # if we found the address for this xml\n",
    "            pass # Skip if we've already found an address for this XML filename\n",
    "        else:\n",
    "            with open(item, \"r\", encoding=\"utf-8\") as file: # Open a file\n",
    "                letterfile = file.readlines() # Les innholdet som linjer\n",
    "                letterfile = \"\".join(letterfile) # Linjene blir kombinert i en variabel\n",
    "            soup = BeautifulSoup(letterfile, features=\"xml\") # It is now soup\n",
    "            #find_address = soup.findAll(\"address\") # Look for addrline element.\n",
    "            #augh,addrKey = \"\",\"\" # Reset\n",
    "            foundSender,foundRecipient = False,False # Reset\n",
    "\n",
    "            ## Code below enables retrieval of an address enclosed in a dateline element.\n",
    "            ## This is understood to be the sender's address.\n",
    "            if foundSender == False: # If a recipient has not been found\n",
    "                find_address = soup.find(\"dateline\") # Look for a dateline element\n",
    "                if find_address: # If there is a dateline element:\n",
    "                    #print(\"Dateline in\",findFileName)\n",
    "                    find_address = find_address.findChild(\"placeName\", recursive=True) # Get the placename\n",
    "                    #print(\"DATELINE\",find_address)\n",
    "                    try: # There are documents with datelines but no locations in them confirmed.\n",
    "                        addrKey = find_address.get('key') # Get the internal ID of the placename\n",
    "                        try:\n",
    "                            addrKey = addrKey.replace(\"pl\",\"\") # Remove \"pl\" prefix\n",
    "                            addrsFoundInXMLs.append(filenamePlain) # Add the filename to the list of XMLs already found\n",
    "                            #placenamedict[filenamePlain][\"location\"] = addrKey\n",
    "                            CHRONODICT[filenamePlain][\"location\"] = addrKey\n",
    "                            foundSender = True\n",
    "                        except:\n",
    "                            print(\"Problem with appending\")\n",
    "\n",
    "                    except:\n",
    "                        if filenamePlain not in xmlswithnoaddress:\n",
    "                            xmlswithnoaddress.append(filenamePlain)\n",
    "                        #print(\"Dateline without address in\",findFileName)\n",
    "                else:\n",
    "                    if filenamePlain not in xmlswithnoaddress:\n",
    "                        xmlswithnoaddress.append(filenamePlain)\n",
    "        i+=1\n",
    "print(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasXMLs == True:\n",
    "    placenamedf = pd.read_excel(\"ID_sted-verdier.xlsx\").dropna(axis=1, how='all').dropna(axis=0, how='all').reset_index(drop=True)\n",
    "    placenamedf = placenamedf.fillna(\"N/A\")\n",
    "    #for item in placenamedict:\n",
    "    for item in CHRONODICT:\n",
    "        if keys_exists(CHRONODICT,item,'location'):\n",
    "            #print(\"\\tSent from:\",CHRONODICT[item]['location'])\n",
    "            #print(item)\n",
    "            try:\n",
    "                placenameSearch = placenamedf[placenamedf['ID'].astype(str) == str(CHRONODICT[item]['location'])]\n",
    "                stedsnavn = placenameSearch[\"sted\"].values[0]\n",
    "                regionnavn = placenameSearch[\"region, nasjonal\"].values[0]\n",
    "                landnavn = placenameSearch[\"land\"].values[0]\n",
    "                kontinent = placenameSearch[\"region, internasjonal\"].values[0]\n",
    "                try:\n",
    "                    if stedsnavn != \"N/A\":\n",
    "                        #print(\"\\tSted:\",stedsnavn)\n",
    "                        CHRONODICT[item]['location'] = stedsnavn\n",
    "                    else:\n",
    "                        if regionnavn != \"N/A\":\n",
    "                            #print(\"\\tRegion:\",regionnavn)\n",
    "                            CHRONODICT[item]['location'] = regionnavn\n",
    "                        else:\n",
    "                            if landnavn != \"N/A\":\n",
    "                                #print(\"\\tLand:\",landnavn)\n",
    "                                CHRONODICT[item]['location'] = landnavn\n",
    "                            else:\n",
    "                                if kontinent != \"N/A\":\n",
    "                                    #print(\"\\tKontinent:\",kontinent)\n",
    "                                    CHRONODICT[item]['location'] = kontinent\n",
    "                                else:\n",
    "                                    print(\"\\tFant ikke stedsnavn.\")\n",
    "                except:\n",
    "                    print(\"\\t\\tCouldn't match, I guess?\")\n",
    "            except:\n",
    "                print(\"\\t\\tNo match for\",str(CHRONODICT[item]['location']))\n",
    "        #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089217f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(CHRONODICT)\n",
    "chronoDF = df.T.fillna(\"N/A\").reset_index(drop=False)\n",
    "# Check for presence of date column (otherwise, would crash if no chronology used)\n",
    "if 'date' not in chronoDF.columns:\n",
    "    chronoDF['date'] = np.nan\n",
    "    print(\"Filled date column with NaN\")\n",
    "# Check for presence of location column (otherwise, would crash if no letter XMLs used)\n",
    "if 'location' not in chronoDF.columns:\n",
    "    chronoDF['location'] = np.nan\n",
    "    print(\"Filled location column with NaN\")\n",
    "chronoDF = chronoDF.rename(columns={\"index\": \"document\"}).sort_values(by=['document']).reset_index(drop=True)\n",
    "places = 0\n",
    "dates = 0\n",
    "items = 0\n",
    "for idx,row in chronoDF.iterrows():\n",
    "    if row['date'] == \"N/A\" and row['location'] == \"N/A\":\n",
    "        print(idx)\n",
    "    else:\n",
    "        #print(idx)\n",
    "        items += 1\n",
    "        if row['date'] != \"N/A\":\n",
    "            #print(row['date'])\n",
    "            dates += 1\n",
    "        if row['location'] != \"N/A\":\n",
    "            #print(row['location'])\n",
    "            places += 1\n",
    "        #print(\"-\")\n",
    "print(places,\"places and\",dates,\"dates added over a total of\",items,\"items.\")\n",
    "chronoDF.to_csv(\"preprocessed.csv\", sep=',', encoding='utf-8',index=False)\n",
    "print(\"Preprocessing complete. Saved to preprocessed.csv. Printing report as Preprocessor Report.txt.\")\n",
    "\n",
    "if i > 0:\n",
    "    pass\n",
    "else:\n",
    "    i=0\n",
    "\n",
    "goodstring = \"MXMLM Preprocessor\"\n",
    "try:\n",
    "    if plainnames:\n",
    "        goodstring += \"\\nChecked \"+str(i)+\" files, of which \"+str(len(plainnames))+\" were identified as letters, of which \"+str(len(addrsFoundInXMLs))+\" had addresses.\"\n",
    "    else:\n",
    "        goodstring += \"\\nCAUTION Checked \"+str(i)+\" (zero?) files, of which \"+str(len(plainnames))+\" were identified as letters, of which \"+str(len(addrsFoundInXMLs))+\" had addresses.\"\n",
    "except:\n",
    "    goodstring+= \"\\nDidn't use placename augmentation?\"\n",
    "goodstring+=\"\\n\"+str(places)+\" places and \"+str(dates)+\" dates added over a total of \"+str(items)+\" items.\"\n",
    "if len(listofbaddies) > 0:\n",
    "    errorstring = \"Bad document IDs from the Chronology that could not be resolved:\\n\"\n",
    "    for x in listofbaddies:\n",
    "        errorstring+=\"\\\"\"+x+\"\\\" \"\n",
    "    errorstring = errorstring.rstrip()\n",
    "else:\n",
    "    errorstring = \"Didn't identify any bad document IDs that couldn't be resolved.\"\n",
    "outputstring = goodstring+\"\\n\"+errorstring\n",
    "\n",
    "with open(\"Preprocessor Report.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(outputstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runFullProcessor == 0:\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    print(\"Proceeding with full data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b3121",
   "metadata": {},
   "source": [
    "# Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05595aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Main file: Correspondence\n",
    "# Decrypting XML files is slow, and doing it while constructing an XML is unneccessarily complex.\n",
    "# Let's just process the XML files here.\n",
    "\n",
    "# Because Correspondence and Register_Tei are slightly different, I'm writing specific code for each despite\n",
    "# the fact that it's somewhat WET programming.\n",
    "\n",
    "if os.path.isfile(\"correspondence.xml\"):\n",
    "    CorrespDict = collections.defaultdict(dict)\n",
    "    print(\"CorrespDict initiated\")\n",
    "    print(\"Melting correspondence.xml\")\n",
    "    with open(\"correspondence.xml\", \"r\", encoding=\"utf-8\") as file: # Open a file\n",
    "        tei = file.readlines() # Les innholdet som linjer\n",
    "        tei = \"\".join(tei) # Linjene blir kombinert i en variabel\n",
    "    soup = BeautifulSoup(tei, from_encoding=\"UTF-8\", features=\"xml\") # It is now soup\n",
    "    # Don't worry about the error screaming about Unicode markup being provided twice\n",
    "    print(\"Correspondence is now soup.\")\n",
    "    comments = 0\n",
    "    commentDocs = 0\n",
    "    for comment in soup.findAll(string=lambda text: isinstance(text, Comment)):\n",
    "        if \"xml:id=\\\"\" in comment:\n",
    "            commentDocs+=1\n",
    "        comment.extract()\n",
    "        comments+=1\n",
    "    if comments > 0:\n",
    "        print(\"Destroyed\",comments,\"<!--comments-->, of which\",commentDocs,\"contained an @XML:ID.\")\n",
    "    # ... and checking it twice.\n",
    "    comments = soup.findAll(string=lambda text: isinstance(text, Comment))\n",
    "    if comments:\n",
    "        print(\"There are still\",len(comments),\"comments present.\")\n",
    "    else:\n",
    "        print(\"All comments destroyed.\")\n",
    "        \n",
    "    print(\"\\nInitializing documentID scan.\")\n",
    "    documentIDs = []\n",
    "    for document in soup.findAll(\"div\", {\"xml:id\":True}):\n",
    "\n",
    "        # Look for the document type assignment.\n",
    "        documentType = document.find(\"list\", {\"type\" : \"objectType\"}).findChild(True, recursive=True)#.attrs['n']\n",
    "        # Checks if the words \"letter\" or \"brev\" appear in the type\n",
    "        if \"brev\" in documentType or \"letter\" in documentType: \n",
    "            # Get the document ID from the <div> element.\n",
    "            documentID = list(document.attrs.values())[0]\n",
    "            documentIDs.append(documentID)\n",
    "    print(\"Acquired\",len(documentIDs),\"documents classed as letters.\\n\")\n",
    "        \n",
    "    # Slight repetition - breaking DRY, I know - but by referencing documentIDs we're 100% only treating letters,\n",
    "    # not wasting time on irrelevant documents\n",
    "    i=1\n",
    "    for eachID in documentIDs:\n",
    "        \n",
    "        i+=1\n",
    "        #MM_K3421\n",
    "        docAuthors = [] # List of authors to be included in the dict.\n",
    "        docAuthorRefs = [] # List of authors' reference URLs.\n",
    "        docRecipients = [] # List of recipients to be included in the dict. \n",
    "\n",
    "        #print(eachID)\n",
    "        # Munch is the recipient of everything in correspondence.xml.\n",
    "        recipient = \"Edvard Munch\"\n",
    "\n",
    "        # Target the document as var \"document\"\n",
    "        document = soup.find(\"div\", {\"xml:id\":eachID})\n",
    "\n",
    "        # Target the author(s) as authorNameList\n",
    "        authorNameList = document.find(\"item\", {\"n\":\"sender\"}).findChildren(True, recursive=True)\n",
    "        X=0\n",
    "        for name in authorNameList:\n",
    "            try:\n",
    "                authorName = authorNameList[X].contents[0]\n",
    "                try:\n",
    "                    targetRef = authorName['target']\n",
    "                except:\n",
    "                    targetRef = \"N/A\"\n",
    "            except:\n",
    "                authorName = \"N/A\"\n",
    "                targetRef = \"N/A\"\n",
    "            \n",
    "            # Data cleaning\n",
    "            authorName = authorName.replace(\",\",\" \")\n",
    "            #authorName = authorName.replace(\";\",\" \")\n",
    "            #authorName = authorName.replace(\"[\",\" \")\n",
    "            #authorName = authorName.replace(\"]\",\" \")\n",
    "            #authorName = authorName.replace(\"?\",\" \")\n",
    "            authorName = re.sub(' +', ' ',authorName)\n",
    "            authorName = authorName.strip()\n",
    "            \n",
    "            \n",
    "            X+=1\n",
    "            docAuthors.append(authorName)\n",
    "            docAuthorRefs.append(targetRef)\n",
    "\n",
    "        statusMessage = str(eachID)+\" \"+str(len(authorNameList))+\" author\"\n",
    "        if len(docAuthors) > 1:\n",
    "            statusMessage += \"s\"\n",
    "        statusMessage+=\" (\"\n",
    "        z = 1\n",
    "        for author in docAuthors:\n",
    "            if author != \"N/A\":\n",
    "                statusMessage+=str(author)\n",
    "            else:\n",
    "                statusMessage+=\"N/A (error?)\"\n",
    "            if z < len(docAuthors):\n",
    "                statusMessage+=\",\"\n",
    "            z+=1\n",
    "        statusMessage+=\") addressed to: \"+str(recipient)\n",
    "\n",
    "\n",
    "        isDocumentUndated = document.find(\"item\", {\"n\":\"undated\"})\n",
    "        if eachID in CHRONODICT:\n",
    "            try:\n",
    "                newdate = CHRONODICT[eachID]['date']\n",
    "                gotDate = True\n",
    "            except:\n",
    "                newdate = \"N/A\"\n",
    "                gotDate = False\n",
    "            try:\n",
    "                place = CHRONODICT[eachID]['location']\n",
    "                gotPlace = True\n",
    "            except:\n",
    "                place = \"N/A\"\n",
    "                gotPlace = False\n",
    "        else:\n",
    "            gotDate = False\n",
    "            gotPlace = False\n",
    "        #if gotDate == False: # Check if we got an ID\n",
    "        if isDocumentUndated:\n",
    "            # Document is straight up undated.\n",
    "            date = \"s.d.\"\n",
    "            datetype = \"N/A\"\n",
    "\n",
    "        else:\n",
    "            #statusMessage+=\"\\n>Dated: \"\n",
    "            isDocumentFromTo = document.find(\"date\", {\"from\":True}) # Does the date element have a from assignment? \n",
    "            # ! Using \"from\" because PN1350 does not have a fromTo attr despite using fromTo. Uses \"from\", though. Works fine.\n",
    "            if isDocumentFromTo: # If it does, and thus has a range (JK, No-MM_T1296 has FROM attr but not a TO attr.)\n",
    "                doesDocumentHaveToDate = document.find(\"date\", {\"to\":True})\n",
    "                #statusMessage+=\"range, \"\n",
    "                if doesDocumentHaveToDate:\n",
    "                    # Both from and to attributes are present.\n",
    "                    fromDate = isDocumentFromTo['from'] # Extract 'from' date. \n",
    "                    toDate = isDocumentFromTo['to'] # Extract 'to' date.\n",
    "                    datetype = \"fromTo\"\n",
    "                    date = str(fromDate)+\"%\"+str(toDate)\n",
    "                    #statusMessage+=datetype+\" \"+str(fromDate)+\"-\"+str(toDate)\n",
    "                else:\n",
    "                    # If the 'from' attribute is present without the 'to', it's interpreted as \"not before this date\".\n",
    "                    date = isDocumentFromTo['from']\n",
    "                    fromDate = isDocumentFromTo['from']\n",
    "                    datetype = \"notBefore\"\n",
    "\n",
    "            else: # If it doesn't:\n",
    "                #statusMessage+=\"instance, \"\n",
    "                yearSent = document.find(\"date\", {\"type\":\"year\",\"when\":True}) # Check for year element\n",
    "                monthSent = document.find(\"date\", {\"type\":\"month\",\"when\":True}) # Check for month element\n",
    "                daySent = document.find(\"date\", {\"type\":\"day\",\"when\":True}) # Check for day element\n",
    "                if yearSent:\n",
    "                    datetype = \"instance\"\n",
    "                    date = yearSent.attrs[\"when\"]\n",
    "                    if monthSent: # Only look for a month if there's a year. That 1 letter with just month/day, tho...\n",
    "                        M = re.sub('[-]', '', monthSent.attrs[\"when\"]) # Strip the random '-' characters in here.\n",
    "                        date+=\"-\"+str(M) # Join month to year by YYYY-MM.\n",
    "                        if daySent: # Only applies if there is a month AND a day. No point having a day if you don't have a month.\n",
    "                            M = re.sub('[-]', '', daySent.attrs[\"when\"]) # Strip the random '-' characters in here, too.\n",
    "                            date+=\"-\"+str(M) # Join day to year-month by YYYY-MM-DD.\n",
    "\n",
    "                else: \n",
    "                # If it doesn't have a year, make one last check\n",
    "                    doesDocumentHaveToDate = document.find(\"date\", {\"to\":True}) # if the date just has a to date...\n",
    "\n",
    "                    if doesDocumentHaveToDate:\n",
    "                    # If the 'to' attribute is present without the 'from', it's interpreted as \"not after this date\".\n",
    "                        datetype = \"notAfter\"\n",
    "                        date = doesDocumentHaveToDate['to']\n",
    "\n",
    "                    else:\n",
    "                    # All else has failed. This data is expunged.\n",
    "                        datetype = \"N/A\"\n",
    "                        date = \"s.d.\"\n",
    "\n",
    "\n",
    "        #statusMessage+=\"\\n\"\n",
    "        if datetype == \"N/A\":\n",
    "            statusMessage+=\"\\n(undated)\"\n",
    "        else:\n",
    "            statusMessage+=\"\\nDate type: \"+str(datetype)+\" (\"+str(date)+\")\"\n",
    "        \n",
    "        if gotDate == True:\n",
    "            statusMessage+=f\" (augmented: {newdate}).\"\n",
    "        if gotPlace == True:\n",
    "            statusMessage+=f\"Place augmentation: {place})\"\n",
    "        print(statusMessage)\n",
    "        CorrespDict[eachID]['authors'] = docAuthors\n",
    "        CorrespDict[eachID]['date'] = date\n",
    "        CorrespDict[eachID]['datetype'] = datetype\n",
    "        CorrespDict[eachID]['recipients'] = recipient\n",
    "        if gotPlace == True:\n",
    "            CorrespDict[eachID]['place'] = place\n",
    "        else:\n",
    "            CorrespDict[eachID]['place'] = \"N/A\"\n",
    "        if gotDate == True:\n",
    "            CorrespDict[eachID]['newdate'] = newdate\n",
    "            CorrespDict[eachID]['newdatetype'] = CHRONODICT[eachID][\"datetype\"]\n",
    "        else:\n",
    "            CorrespDict[eachID]['newdate'] = \"s.d.\"\n",
    "            CorrespDict[eachID]['newdatetype'] = \"N/A\"\n",
    "        json_object = json.dumps(CorrespDict, indent=4)\n",
    "\n",
    "        with open(\"correspondence.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "else:\n",
    "    print(\"No correspondence.xml file provided. MXML will not munch letters to Munch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRONODICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0e331",
   "metadata": {},
   "source": [
    "CorrespDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7d666",
   "metadata": {},
   "source": [
    "for item in CorrespDict:\n",
    "    i=1\n",
    "    print(item+\",\",CorrespDict[item]['date'])\n",
    "    for author in CorrespDict[item]['authors']:\n",
    "        print(i,author)\n",
    "        i+=1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ce969",
   "metadata": {},
   "source": [
    "# Main Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"register_tei.xml\"):\n",
    "    RegDict = collections.defaultdict(dict)\n",
    "    print(\"RegDict initiated\")\n",
    "    print(\"Melting register_tei.xml\")\n",
    "    with open(\"register_tei.xml\", \"r\", encoding=\"utf-8\") as file: # Open a file\n",
    "        tei = file.readlines() # Les innholdet som linjer\n",
    "        tei = \"\".join(tei) # Linjene blir kombinert i en variabel\n",
    "    soup = BeautifulSoup(tei, from_encoding=\"UTF-8\", features=\"xml\") # It is now soup\n",
    "    # Don't worry about the error screaming about Unicode markup being provided twice\n",
    "    print(\"Registry is now soup.\")\n",
    "    comments = 0\n",
    "    commentDocs = 0\n",
    "    for comment in soup.findAll(string=lambda text: isinstance(text, Comment)):\n",
    "        if \"xml:id=\\\"\" in comment:\n",
    "            commentDocs+=1\n",
    "        comment.extract()\n",
    "        comments+=1\n",
    "    if comments > 0:\n",
    "        print(\"Destroyed\",comments,\"<!--comments-->, of which\",commentDocs,\"contained an @XML:ID.\")\n",
    "    # ... and checking it twice.\n",
    "    comments = soup.findAll(string=lambda text: isinstance(text, Comment))\n",
    "    if comments:\n",
    "        print(\"There are still\",len(comments),\"comments present.\")\n",
    "    else:\n",
    "        print(\"All comments destroyed.\")\n",
    "        \n",
    "    print(\"\\nInitializing documentID scan.\")\n",
    "    documentIDs = []\n",
    "    for document in soup.findAll(\"div\", {\"xml:id\":True}):\n",
    "\n",
    "        # Look for the document type assignment.\n",
    "        documentType = document.find(\"list\", {\"type\" : \"objectType\"}).findChild(True, recursive=True)#.attrs['n']\n",
    "        # Checks if the words \"letter\" or \"brev\" appear in the type\n",
    "        if \"brev\" in documentType or \"letter\" in documentType: \n",
    "            # Get the document ID from the <div> element.\n",
    "            documentID = list(document.attrs.values())[0]\n",
    "            documentIDs.append(documentID)\n",
    "    print(\"Acquired\",len(documentIDs),\"documents classed as letters.\\n\")\n",
    "        \n",
    "    # Slight repetition - breaking DRY, I know - but by referencing documentIDs we're 100% only treating letters,\n",
    "    # not wasting time on irrelevant documents\n",
    "    i=1\n",
    "    for eachID in documentIDs:\n",
    "    \n",
    "        \n",
    "        i+=1\n",
    "        #MM_K3421\n",
    "        docAuthors = [] # List of authors to be included in the dict.\n",
    "        docRecipRefs = [] # List of authors' reference URLs.\n",
    "        docRecipients = [] # List of recipients to be included in the dict. \n",
    "\n",
    "        #print(eachID)\n",
    "        # Munch is the author of everything in the registry.\n",
    "        author = \"Edvard Munch\"\n",
    "\n",
    "        # Target the document as var \"document\"\n",
    "        document = soup.find(\"div\", {\"xml:id\":eachID})\n",
    "\n",
    "        # Target the recipients(s) as recipNameList\n",
    "        recipNameList = document.find(\"item\", {\"n\":\"recipient\"}).findChildren(True, recursive=True)\n",
    "        X=0\n",
    "        for name in recipNameList:\n",
    "            try:\n",
    "                recipName = recipNameList[X].contents[0]\n",
    "                try:\n",
    "                    targetRef = recipName['target']\n",
    "                except:\n",
    "                    targetRef = \"N/A\"\n",
    "            except:\n",
    "                recipName = \"N/A\"\n",
    "                targetRef = \"N/A\"\n",
    "            \n",
    "            # Data cleaning\n",
    "            recipName = recipName.replace(\",\",\"&#44;\")\n",
    "            recipName = recipName.strip()\n",
    "            #recipName = recipName.replace(\";\",\" \")\n",
    "            #recipName = recipName.replace(\"[\",\" \")\n",
    "            #recipName = recipName.replace(\"]\",\" \")\n",
    "            #recipName = recipName.replace(\"?\",\" \")\n",
    "            recipName = re.sub(' +', ' ',recipName)\n",
    "            if eachID == \"No-MM_N0725\":\n",
    "                print(name,recipName)\n",
    "            \n",
    "            X+=1\n",
    "            docRecipients.append(recipName)\n",
    "            docRecipRefs.append(targetRef)\n",
    "\n",
    "        statusMessage = str(eachID)+\" \"+str(len(docRecipients))+\" recipients\"\n",
    "        if len(docRecipients) > 1:\n",
    "            statusMessage += \"s\"\n",
    "        statusMessage+=\" (\"\n",
    "        z = 1\n",
    "        for recipient in docRecipients:\n",
    "            if recipient != \"N/A\":\n",
    "                statusMessage+=str(recipient)\n",
    "            else:\n",
    "                statusMessage+=\"N/A (error?)\"\n",
    "            if z < len(docRecipients):\n",
    "                statusMessage+=\",\"\n",
    "            z+=1\n",
    "        statusMessage+=\") from: \"+str(author)\n",
    "\n",
    "\n",
    "        isDocumentUndated = document.find(\"item\", {\"n\":\"undated\"})\n",
    "        \n",
    "        if eachID in CHRONODICT:\n",
    "            try:\n",
    "                newdate = CHRONODICT[eachID]['date']\n",
    "                gotDate = True\n",
    "            except:\n",
    "                newdate = \"N/A\"\n",
    "                gotDate = False\n",
    "            try:\n",
    "                place = CHRONODICT[eachID]['location']\n",
    "                gotPlace = True\n",
    "            except:\n",
    "                place = \"N/A\"\n",
    "                gotPlace = False\n",
    "        else:\n",
    "            gotDate = False\n",
    "            gotPlace = False\n",
    "        \n",
    "        \n",
    "        if isDocumentUndated:\n",
    "            # Document is straight up undated.\n",
    "            date = \"s.d.\"\n",
    "            datetype = \"N/A\"\n",
    "\n",
    "        else:\n",
    "            #statusMessage+=\"\\n>Dated: \"\n",
    "            isDocumentFromTo = document.find(\"date\", {\"from\":True}) # Does the date element have a from assignment? \n",
    "            # ! Using \"from\" because PN1350 does not have a fromTo attr despite using fromTo. Uses \"from\", though. Works fine.\n",
    "            if isDocumentFromTo: # If it does, and thus has a range (JK, No-MM_T1296 has FROM attr but not a TO attr.)\n",
    "                doesDocumentHaveToDate = document.find(\"date\", {\"to\":True})\n",
    "                #statusMessage+=\"range, \"\n",
    "                if doesDocumentHaveToDate:\n",
    "                    # Both from and to attributes are present.\n",
    "                    fromDate = isDocumentFromTo['from'] # Extract 'from' date. \n",
    "                    toDate = isDocumentFromTo['to'] # Extract 'to' date.\n",
    "                    datetype = \"fromTo\"\n",
    "                    date = str(fromDate)+\"%\"+str(toDate)\n",
    "                    #statusMessage+=datetype+\" \"+str(fromDate)+\"-\"+str(toDate)\n",
    "                else:\n",
    "                    # If the 'from' attribute is present without the 'to', it's interpreted as \"not before this date\".\n",
    "                    date = isDocumentFromTo['from']\n",
    "                    fromDate = isDocumentFromTo['from']\n",
    "                    datetype = \"notBefore\"\n",
    "\n",
    "            else: # If it doesn't:\n",
    "                #statusMessage+=\"instance, \"\n",
    "                yearSent = document.find(\"date\", {\"type\":\"year\",\"when\":True}) # Check for year element\n",
    "                monthSent = document.find(\"date\", {\"type\":\"month\",\"when\":True}) # Check for month element\n",
    "                daySent = document.find(\"date\", {\"type\":\"day\",\"when\":True}) # Check for day element\n",
    "                if yearSent:\n",
    "                    datetype = \"instance\"\n",
    "                    date = yearSent.attrs[\"when\"]\n",
    "                    if monthSent: # Only look for a month if there's a year. That 1 letter with just month/day, tho...\n",
    "                        M = re.sub('[-]', '', monthSent.attrs[\"when\"]) # Strip the random '-' characters in here.\n",
    "                        date+=\"-\"+str(M) # Join month to year by YYYY-MM.\n",
    "                        if daySent: # Only applies if there is a month AND a day. No point having a day if you don't have a month.\n",
    "                            M = re.sub('[-]', '', daySent.attrs[\"when\"]) # Strip the random '-' characters in here, too.\n",
    "                            date+=\"-\"+str(M) # Join day to year-month by YYYY-MM-DD.\n",
    "\n",
    "                else: \n",
    "                # If it doesn't have a year, make one last check\n",
    "                    doesDocumentHaveToDate = document.find(\"date\", {\"to\":True}) # if the date just has a to date...\n",
    "\n",
    "                    if doesDocumentHaveToDate:\n",
    "                    # If the 'to' attribute is present without the 'from', it's interpreted as \"not after this date\".\n",
    "                        datetype = \"notAfter\"\n",
    "                        date = doesDocumentHaveToDate['to']\n",
    "\n",
    "                    else:\n",
    "                    # All else has failed. This data is expunged.\n",
    "                        datetype = \"N/A\"\n",
    "                        date = \"s.d.\"\n",
    "\n",
    "\n",
    "        #statusMessage+=\"\\n\"\n",
    "        if datetype == \"N/A\":\n",
    "            statusMessage+=\"\\n(undated)\"\n",
    "        else:\n",
    "            statusMessage+=\"\\nDate type: \"+str(datetype)+\" (\"+str(date)+\")\"\n",
    "        print(statusMessage)\n",
    "        RegDict[eachID]['authors'] = author\n",
    "        RegDict[eachID]['date'] = date\n",
    "        RegDict[eachID]['datetype'] = datetype\n",
    "        RegDict[eachID]['recipients'] = docRecipients\n",
    "        if gotPlace == True:\n",
    "            RegDict[eachID]['place'] = place\n",
    "        else:\n",
    "            RegDict[eachID]['place'] = \"N/A\"\n",
    "        if gotDate == True:\n",
    "            RegDict[eachID]['newdate'] = newdate\n",
    "            RegDict[eachID]['newdatetype'] = CHRONODICT[eachID][\"datetype\"]\n",
    "        else:\n",
    "            RegDict[eachID]['newdate'] = \"s.d.\"\n",
    "            RegDict[eachID]['newdatetype'] = \"N/A\"\n",
    "        json_object = json.dumps(RegDict, indent=4)\n",
    "        \n",
    "        document.decompose() # \n",
    "        \n",
    "        with open(\"registry.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "else:\n",
    "    print(\"No correspondence.xml file provided. MXML will not munch letters to Munch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b7f21",
   "metadata": {},
   "source": [
    "if os.path.exists(\"preprocessed.csv\"):\n",
    "    print(\"MxmlM located and will use preprocessed data (dates, places).\")\n",
    "    flagPreprocessor = True # We're using preprocessed data\n",
    "    dfPP = pd.read_csv(\"preprocessed.csv\",sep=\",\").fillna(\"N/A\") # Fill up NaN with N/A\n",
    "    \n",
    "    augments = collections.defaultdict(dict) # New dict to hold the values\n",
    "    \n",
    "    ppdocs = [] # Simple list of documents that have additional information from preprocessing\n",
    "    docIDs_placenames = [] # Quickly identify what docids are getting new places\n",
    "    docIDs_singledates = [] # Quickly identify what docids are getting new SINGLE dates\n",
    "    docIDs_fromtodates = [] # Quickly identify what docids are getting FROM-TO RANGE dates\n",
    "    \n",
    "    for doc in dfPP['document']:\n",
    "        ppdocs.append(doc) # Populate list of docs that are getting augmented\n",
    "        \n",
    "    \n",
    "    for idx,row in dfPP.iterrows():\n",
    "        dat = row['date']\n",
    "        loc = row['location']\n",
    "        doc = row['document']\n",
    "        if dat == \"N/A\":\n",
    "            # if date is N/A, we're not doing anything with it\n",
    "            pass\n",
    "        elif \"%\" in dat:\n",
    "            # Date range - IMPORTANT split the date into two parts if there's a separator (%)\n",
    "            docIDs_fromtodates.append(doc)\n",
    "            #augments[doc]['fromdate'] = fromdate\n",
    "        else:\n",
    "            docIDs_singledates.append(doc)\n",
    "            #augments[doc]['date'] = dat\n",
    "        if loc == \"N/A\":\n",
    "            pass\n",
    "        else:\n",
    "            docIDs_placenames.append(doc)\n",
    "        augments[doc]['sender'] = loc\n",
    "        augments[doc]['date'] = dat\n",
    "else:\n",
    "    print(\"WARNING MxmlM is running WITHOUT preprocessed data (dates, places)!\")\n",
    "    flagPreprocessor = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12b87d",
   "metadata": {},
   "source": [
    "for item in RegDict:\n",
    "    if RegDict[item]['newdate'] != \"s.d.\":\n",
    "        if RegDict[item]['date'] != RegDict[item]['newdate']:\n",
    "            try:\n",
    "                index = dfCombo.loc[item]\n",
    "            except:\n",
    "                print(\"Failed to locate\",item)\n",
    "            index['date'] = RegDict[item]['newdate']\n",
    "            index['datetype'] = \n",
    "            print(item,RegDict[item]['date'],RegDict[item]['datetype'],\"->\",RegDict[item]['newdate'],RegDict[item]['newdatetype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(RegDict).T#.reset_index(drop=False)\n",
    "df2 = pd.DataFrame.from_dict(CorrespDict).T#.reset_index(drop=False)\n",
    "dfCombo = df1.append(df2).reset_index(drop=False).rename(columns={'index':'document'})\n",
    "dfComboFull = dfCombo.copy() # Full copy including old and new dates.\n",
    "#for idx,row in dfCombo.iterrows():\n",
    "#    newdatetype,newdate = row['date'],row['newdate']\n",
    " #   datetype,newdatetype = row['datetype'],row['newdatetype']\n",
    " #   authors,recipients = row['authors'],row['recipients']\n",
    " ##   if newdate == \"s.d.\":\n",
    "  #      dfCombo.iloc[idx][\"newdate\"] = date\n",
    " #   if newdatetype == \"N/A\":\n",
    "  #      dfCombo.iloc[idx][\"newdatetype\"] = datetype\n",
    "    # We're going to keep the lists alive for use in creating the CMIF. Practical.\n",
    "    #if authors == \"Edvard Munch\":\n",
    "    #    dfCombo.iloc[idx]['recipients'] = ','.join(recipients)\n",
    "    #else:\n",
    "    #    dfCombo.iloc[idx]['authors'] = ','.join(authors)\n",
    "#dfCombo = dfCombo.drop([\"date\",\"datetype\"],axis=1).rename(columns={'index':'document','newdate': 'date','newdatetype':'datetype'})\n",
    "\n",
    "result = dfCombo.to_json(orient=\"index\")\n",
    "parsed = json.loads(result)\n",
    "dumped = json.dumps(parsed, indent=4)\n",
    "with open(\"MXMLM_Output_Combined.json\", \"w\") as outfile:\n",
    "    outfile.write(dumped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586586bc",
   "metadata": {},
   "source": [
    "newDates,newDateTypes,newPlaces,i = [],[],[],0\n",
    "for idx,row in dfCombo.iterrows():\n",
    "    document,date,datetype = row['index'],row['date'],row['datetype']\n",
    "    if document in CHRONODICT:\n",
    "        print(document)\n",
    "        try:\n",
    "            newDate = CHRONODICT[document]['date']\n",
    "            #newDate\n",
    "            if \"%\" in newDate:\n",
    "                newDateType = \"fromTo\"\n",
    "            else:\n",
    "                newDateType = \"instance\"\n",
    "            #print(newDate,newDateType)\n",
    "        except:\n",
    "            newDate,newDateType = \"N/A\",\"N/A\" # No date\n",
    "        try:\n",
    "            newPlace = CHRONODICT[document]['location']\n",
    "        except:\n",
    "            newPlace = \"N/A\" # No location\n",
    "        newDates.append(newDate)\n",
    "        newDateTypes.append(newDateType)\n",
    "        newPlaces.append(newPlace)\n",
    "        i+=1\n",
    "    else:\n",
    "        newDates.append(date)\n",
    "        newDateTypes.append(datetype)\n",
    "        newPlaces.append(\"N/A\")\n",
    "print(f\"{i} out of {len(CHRONODICT)} entries passed on\")\n",
    "dfCombo['dates'] = newDates\n",
    "dfCombo['datetypes'] = newDateTypes\n",
    "dfCombo['place'] = newPlaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da97a7",
   "metadata": {},
   "source": [
    "for idx,row in dfCombo.iterrows():\n",
    "    if row['date'] != row['dates']:\n",
    "        print(row['index'],row['date'], row['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCombo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181a945",
   "metadata": {},
   "source": [
    "# Production Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3233baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser # Used to easily get statements from the config file\n",
    "from datetime import date\n",
    "\n",
    "version = \"2.0\" # Describes the \"program's\" state of completion and versioning.\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "cmifTitle = config.get(\"statements\", \"cmifTitle\")\n",
    "editorName = config.get(\"statements\", \"editorName\")\n",
    "editorMail = config.get(\"statements\", \"editorMail\")\n",
    "cmifUid = config.get(\"statements\", \"cmifUid\")\n",
    "publisherURL = config.get(\"statements\", \"publisherURL\")\n",
    "publisherName = config.get(\"statements\", \"publisherName\")\n",
    "cmifURL = config.get(\"statements\", \"cmifURL\")\n",
    "typeOfBibl = config.get(\"statements\", \"typeOfBibl\")\n",
    "publicationStatementFull = config.get(\"statements\", \"publicationStatementFull\")\n",
    "cmifTitle += \" \"+str(version) # Title of resulting CMIF\n",
    "\n",
    "today = date.today() # Sett dato i dag\n",
    "today = today.strftime(\"%Y-%m-%d\") # Formater dato\n",
    "currVer = version+\" \"+today\n",
    "\n",
    "previouslyRun = \"Last executed code was version \"+str(currVer)+\". All OUTPUT files are current to that version on that date.\\n\"+str(cmifUid)+\".\"\n",
    "print(\"Version\",currVer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CMIF boilerplate object\n",
    "CMIFstring = '<?xml-model href=\"https://raw.githubusercontent.com/TEI-Correspondence-SIG/CMIF/master/schema/cmi-customization.rng\" type=\"application/xml\" schematypens=\"http://relaxng.org/ns/structure/1.0\"?><TEI xmlns=\"http://www.tei-c.org/ns/1.0\"><teiHeader><fileDesc><titleStmt><title>'+str(cmifTitle)+'</title><editor>'+str(editorName)+'<email>'+str(editorMail)+'</email></editor></titleStmt><publicationStmt><publisher><ref target=\"'+str(publisherURL)+'\">'+str(publisherName)+'</ref></publisher><idno type=\"url\">'+str(cmifURL)+'</idno> <date when=\"'+str(today)+'\"/><availability><licence target=\"https://creativecommons.org/licenses/by/4.0/\">This file is licensed under the terms of the Creative-Commons-License CC-BY 4.0</licence></availability></publicationStmt><sourceDesc><bibl type=\"'+str(typeOfBibl)+'\" xml:id=\"'+str(cmifUid)+'\">'+str(publicationStatementFull)+'</bibl></sourceDesc></fileDesc><profileDesc><dummy/></profileDesc></teiheader><body><p/></body></text></tei>'\n",
    "CMIF = BeautifulSoup(CMIFstring,\"xml\") # Read as XML, not HTML\n",
    "\n",
    "profileDescElement = CMIF.find('profileDesc') # Target correspondence wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in dfCombo.iterrows():\n",
    "    document,date,datetype,authors,recipients,place = row['document'],row['date'],row['datetype'],row['authors'],row['recipients'],row['place']\n",
    "    metadata = [date,datetype,authors,recipients,place]\n",
    "    if authors != \"Edvard Munch\":\n",
    "        nAuthor = ' & '.join(authors)\n",
    "    else:\n",
    "        nAuthor = authors\n",
    "    if recipients != \"Edvard Munch\":\n",
    "        nRecip = ' & '.join(recipients)\n",
    "    else:\n",
    "        nRecip = recipients\n",
    "    \n",
    "    print(f\"{document}\\tby {nAuthor} to {nRecip}\\nDated {date} ({datetype}), {place}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600e425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
